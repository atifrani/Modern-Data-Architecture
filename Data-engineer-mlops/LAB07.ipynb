{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "nh3npiexphrza4elch3x",
   "authorId": "1383772989750",
   "authorName": "AXEL",
   "authorEmail": "axel@logbrain.fr",
   "sessionId": "26d9f183-f291-449e-af5b-14c20c65c82f",
   "lastEditTime": 1771239339708
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c37393-ab08-4d43-8a74-7a0811ea68d2",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "### Setup du nootebook:"
  },
  {
   "cell_type": "code",
   "id": "b3124f2e-0cba-4c39-8fb0-8fb4f41062f8",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "CREATE DATABASE IF NOT EXISTS ML_LAB_DB;\n\nUSE DATABASE ML_LAB_DB;\n\nCREATE SCHEMA IF NOT EXISTS ML_SCHEMA;\n\nUSE SCHEMA ML_SCHEMA;\n\n-- create csv format\nCREATE FILE FORMAT IF NOT EXISTS CSVFORMAT \n    SKIP_HEADER = 1 \n    TYPE = 'CSV';\n\n-- create external stage with the csv format to stage the diamonds dataset\nCREATE STAGE IF NOT EXISTS DIAMONDS_ASSETS \n    FILE_FORMAT = CSVFORMAT \n    URL = 's3://logbrain-datasets/ml/diamonds.csv';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "06c1c3af-d283-4399-a778-9e2cd7e94133",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "### 1. Data Ingestion:\n\nLe jeu de données **diamonds** a été largement utilisé en science des données et en apprentissage automatique. Nous l’utiliserons pour démontrer les transformateurs natifs de data science de Snowflake en termes de fonctionnalités de base de données et de compatibilité avec Spark et Pandas, en utilisant des données non synthétiques et statistiquement pertinentes, bien connues de la communauté du machine learning.\n\n### Import Libraries"
  },
  {
   "cell_type": "code",
   "id": "138c9f0b-4274-43f5-8f34-96e5320559ec",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# Snowpark for Python\nfrom snowflake.snowpark.types import DoubleType\nimport snowflake.snowpark.functions as F",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8aafd0cc-922a-4ae8-a257-ba4a66b3cf6e",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "### Configuration et établissement d’une connexion à Snowflake\n\nLes notebooks établissent une session Snowpark dans le notebook. Nous utilisant un entrepôt de données (warehouse), une base de données et un schéma tout au long de ce tutoriel."
  },
  {
   "cell_type": "code",
   "id": "fa887ccc-fb76-452e-a189-4559a8899880",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# Get Snowflake Session object\nsession = get_active_session()\nsession.sql_simplifier_enabled = True\n\n# Add a query tag to the session.\nsession.query_tag = {\"origin\":\"Axel_T\", \n                     \"name\":\"Ml_diamands\", \n                     \"version\":{\"major\":1, \"minor\":0,},\n                     \"attributes\":{\"is_quickstart\":1}}\n\n# Current Environment Details\nprint('Connection Established with the following parameters:')\nprint('User      : {}'.format(session.get_current_user()))\nprint('Role      : {}'.format(session.get_current_role()))\nprint('Database  : {}'.format(session.get_current_database()))\nprint('Schema    : {}'.format(session.get_current_schema()))\nprint('Warehouse : {}'.format(session.get_current_warehouse()))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d6284dc-2c6a-4123-a9f2-0a70023bdc47",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "### Utiliser Snowpark Read DataFrame pour charger les données depuis le fichier CSV diamonds stocké dans un stage externe\n\nDans le début de ce notebook, nous avons placé le fichier diamonds.csv dans un stage à partir d’un bucket S3 externe. Nous pouvons maintenant le charger."
  },
  {
   "cell_type": "code",
   "id": "70f5a0f1-4230-4cd0-8f8f-ad9853c6d5b8",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "# Create a Snowpark DataFrame that is configured to load data from the CSV file\n# We can now infer schema from CSV files.\ndiamonds_df = session.read.options({\"field_delimiter\": \",\",\n                                    \"field_optionally_enclosed_by\": '\"',\n                                    \"infer_schema\": True,\n                                    \"parse_header\": True}).csv(\"@DIAMONDS_ASSETS\")\n\ndiamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42d4cbf7-92bf-494e-90aa-a8bab9b3f3b6",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# Look at descriptive stats on the DataFrame\ndiamonds_df.describe()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28a26017-383b-44ec-9a73-f2da4549c024",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "diamonds_df.columns",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15a1c2b2-a47e-42dd-81b8-e56ed74626db",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "### Nettoyage des données\n\nTout d’abord, mettons les en-têtes en majuscules à l’aide des opérations Snowpark DataFrame afin de les standardiser avant l’écriture des colonnes dans une table Snowflake."
  },
  {
   "cell_type": "code",
   "id": "309451cf-8a73-411d-919f-5f31084a379d",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "# Force headers to uppercase\nfor colname in diamonds_df.columns:\n    if colname == '\"table\"':\n       new_colname = \"TABLE_PCT\"\n    else:\n        new_colname = str.upper(colname)\n    diamonds_df = diamonds_df.with_column_renamed(colname, new_colname)\n\ndiamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a1d032e-1acc-4714-a74d-92d97dbe6c03",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "Next, we standardize the category formatting for **CUT** using Snowpark DataFrame operations.\n\nThis way, when we write to a Snowflake table, there will be no inconsistencies in how the Snowpark DataFrame will read in the category names. Secondly, the feature transformations on categoricals will be easier to encode."
  },
  {
   "cell_type": "code",
   "id": "b3432f0a-97a1-46be-8d43-8fec006bad91",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "def fix_values(columnn):\n    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n\nfor col in [\"CUT\"]:\n    diamonds_df = diamonds_df.with_column(col, fix_values(col))\n\ndiamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af4e54cd-e12f-432e-ad7e-3d5ffa10c453",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "Vérifier le schéma."
  },
  {
   "cell_type": "code",
   "id": "5d9c3574-e60f-4ac1-b018-1accdfba864f",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "list(diamonds_df.schema)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa771b26-e9e6-4688-b7d2-cd1ca528a3b7",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "Enfin, convertissons les types décimaux en **DoubleType()**, car **DecimalType()** n’est pas pris en charge par Snowflake ML pour le moment."
  },
  {
   "cell_type": "code",
   "id": "54aab270-041b-48af-8de8-334b7fd19540",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "for colname in [\"CARAT\", \"X\", \"Y\", \"Z\", \"DEPTH\", \"TABLE_PCT\"]:\n    diamonds_df = diamonds_df.with_column(colname, diamonds_df[colname].cast(DoubleType()))\n\ndiamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29b462ab-84d7-4d10-befd-34aae74bee1f",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "Écrire les données nettoyées dans une table Snowflake."
  },
  {
   "cell_type": "code",
   "id": "f5ab3924-11cd-4f31-add9-37d63c4c3ccf",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": "diamonds_df.write.mode('overwrite').save_as_table('diamonds')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "324d0c03-9d53-45c1-a217-354126391c5c",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "Maintenant, nous allons effectuer des transformations de données avec l’API de prétraitement (Preprocessing) de Snowflake ML pour l’ingénierie des variables (feature engineering).\n"
  },
  {
   "cell_type": "markdown",
   "id": "9f043e61-8825-48fa-a7eb-9fe6c81fa187",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "### 2. Transformations des variables ML\n\nNous allons parcourir plusieurs transformations incluses dans l’API de prétraitement de Snowflake ML.\nNous construirons également un pipeline de prétraitement qui sera utilisé dans la modélisation ML.\n\nRemarque : toutes les transformations de variables réalisées avec Snowflake ML sont des opérations distribuées, au même titre que les opérations Snowpark DataFrame.\n"
  },
  {
   "cell_type": "markdown",
   "id": "acb635c5-58c0-4f3e-84f4-e683fb3ec8ce",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "### Import Libraries"
  },
  {
   "cell_type": "code",
   "id": "7c424d1a-5860-495d-adf5-716fb803916b",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": "\n# Snowpark for Python\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.types import DecimalType\n\n# Snowflake ML\nimport snowflake.ml.modeling.preprocessing as snowml\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.metrics.correlation import correlation\n\n# Data Science Libs\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Misc\nimport json\nimport joblib\n\n# warning suppresion\nimport warnings; warnings.simplefilter('ignore')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c96d8067-b9d3-4790-b6e8-ff2d2d6d1a55",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "### Charegement des doonées:\nNous allons charger les données à partir de la table snowflake **DIAMONDS**"
  },
  {
   "cell_type": "code",
   "id": "2505ac50-4e52-4c10-9065-3c531e51c097",
   "metadata": {
    "language": "sql",
    "name": "cell30"
   },
   "outputs": [],
   "source": "USE DATABASE ML_LAB_DB;\n\nUSE SCHEMA ML_SCHEMA;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5e42e27-c734-4805-82e2-f8c0b9c9248b",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": "# Get Snowflake Session object\nsession = get_active_session()\nsession.sql_simplifier_enabled = True\n\n# Add a query tag to the session.\nsession.query_tag = {\"origin\":\"Axel_T\", \n                     \"name\":\"Ml_diamands\", \n                     \"version\":{\"major\":1, \"minor\":0,},\n                     \"attributes\":{\"is_quickstart\":1}}\n\n# Current Environment Details\nprint('Connection Established with the following parameters:')\nprint('User      : {}'.format(session.get_current_user()))\nprint('Role      : {}'.format(session.get_current_role()))\nprint('Database  : {}'.format(session.get_current_database()))\nprint('Schema    : {}'.format(session.get_current_schema()))\nprint('Warehouse : {}'.format(session.get_current_warehouse()))\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c09aa2d3-3ca4-407c-877c-3dc03b81045a",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "# First, we read in the data from a Snowflake table into a Snowpark DataFrame\n# **Change this only if you named your table something else in the data ingest notebook **\ndiamonds_df = session.table(\"DIAMONDS\")\n\ndiamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8beea138-2875-4f68-82a2-58ac0e9dff34",
   "metadata": {
    "name": "cell27",
    "collapsed": false
   },
   "source": "### Transformations des variables\n\nNous allons illustrer ici quelques fonctions de transformation.\n\nUtilisons le MinMaxScaler pour normaliser la colonne CARAT.\n\nOn applique une transformation mathématique à la variable **CARAT** (qui représente le poids du diamant) afin de ramener ses valeurs dans une plage standardisée, généralement **entre 0 et 1**.\n\n### Principe du MinMaxScaler\n\nLe MinMaxScaler transforme chaque valeur selon la formule suivante :\n\n[\nx_{normalisé} ={x - x_{min}} / {x_{max} - x_{min}}\n]\n\n* **x** : valeur originale\n* **xmin** : valeur minimale de la colonne\n* **xmax** : valeur maximale de la colonne\n\nAinsi :\n\n* la plus petite valeur devient **0**\n* la plus grande valeur devient **1**\n* les autres valeurs sont proportionnellement réparties entre 0 et 1\n\n### Pourquoi faire cela ?\n\n* Mettre les variables sur la **même échelle**\n* Améliorer la performance de certains algorithmes de machine learning\n* Éviter qu’une variable avec de grandes valeurs numériques domine les autres\n\nEn résumé, cela permet d’uniformiser l’échelle des données avant l’entraînement d’un modèle.\n\n\n"
  },
  {
   "cell_type": "code",
   "id": "c937f935-4135-4a60-9cc0-43fa9c0223c3",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": "# Normalize the CARAT column\nsnowml_mms = snowml.MinMaxScaler(input_cols=[\"CARAT\"], output_cols=[\"CARAT_NORM\"])\nnormalized_diamonds_df = snowml_mms.fit(diamonds_df).transform(diamonds_df)\n\n# Reduce the number of decimals\nnew_col = normalized_diamonds_df.col(\"CARAT_NORM\").cast(DecimalType(7, 6))\nnormalized_diamonds_df = normalized_diamonds_df.with_column(\"CARAT_NORM\", new_col)\n\nnormalized_diamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f88e6817-3343-4baf-b3fb-c9964d0355c3",
   "metadata": {
    "name": "cell31",
    "collapsed": false
   },
   "source": "Utilisons l’**OrdinalEncoder** pour transformer **COLOR** et **CLARITY** de variables catégorielles en valeurs numériques afin qu’elles soient plus exploitables.\n\nL’OrdinalEncoder est une méthode de transformation qui permet de convertir des variables catégorielles (texte) en valeurs numériques.\n\nDans ce cas, les colonnes COLOR et CLARITY contiennent des catégories (ex. : D, E, F pour la couleur ; IF, VVS1, VS2, etc. pour la pureté). Les algorithmes de machine learning ne peuvent pas traiter directement du texte, ils nécessitent des nombres."
  },
  {
   "cell_type": "code",
   "id": "982ce8c0-bf3a-4c2e-9d58-884918cf47e8",
   "metadata": {
    "language": "python",
    "name": "cell32"
   },
   "outputs": [],
   "source": "# Encode CUT and CLARITY preserve ordinal importance\ncategories = {\n    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n}\nsnowml_oe = snowml.OrdinalEncoder(input_cols=[\"CUT\", \"CLARITY\"], output_cols=[\"CUT_OE\", \"CLARITY_OE\"], categories=categories)\nord_encoded_diamonds_df = snowml_oe.fit(normalized_diamonds_df).transform(normalized_diamonds_df)\n\n# Show the encoding\nprint(snowml_oe._state_pandas)\n\nord_encoded_diamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7a203afc-2546-463a-ab16-9c9d96715b1e",
   "metadata": {
    "name": "cell33",
    "collapsed": false
   },
   "source": "Utilisons le **OneHotEncoder** pour transformer les colonnes catégorielles en colonnes numériques.\n\nCela est davantage à des fins d’illustration. L’utilisation de l’**OrdinalEncoder** est plus pertinente pour le jeu de données *diamonds*, puisque **CARAT**, **COLOR** et **CLARITY** suivent tous un ordre naturel de classement.\n\n### Explication de cette étape\n\n### 1. Pourquoi utiliser le OneHotEncoder ?\n\nLe **OneHotEncoder** permet de transformer une variable catégorielle (texte) en plusieurs colonnes numériques binaires (0 ou 1).\n\nAu lieu d’attribuer un numéro à chaque catégorie (comme avec l’OrdinalEncoder), il crée **une colonne par catégorie**.\n\n---\n\n#### Exemple\n\nSupposons que la colonne **COLOR** contient :\n\nD, E, F\n\nAprès One-Hot Encoding :\n\n| COLOR_D | COLOR_E | COLOR_F |\n| ------- | ------- | ------- |\n| 1       | 0       | 0       |\n| 0       | 1       | 0       |\n| 0       | 0       | 1       |\n\nChaque ligne aura un 1 uniquement dans la colonne correspondant à sa catégorie.\n\n---\n\n### 2. Pourquoi est-ce “à des fins d’illustration” ?\n\nDans le dataset **diamonds**, certaines variables suivent un **ordre naturel** :\n\n* **CARAT** → poids croissant\n* **COLOR** → qualité ordonnée (D meilleure que E, etc.)\n* **CLARITY** → niveau de pureté ordonné\n\nLe OneHotEncoder **ne conserve pas la notion d’ordre**.\nToutes les catégories sont traitées comme indépendantes, sans hiérarchie.\n\n### Résumé\n\n* **OneHotEncoder** : transforme les catégories en colonnes binaires, sans notion d’ordre.\n* **OrdinalEncoder** : transforme les catégories en nombres en respectant un ordre.\n* Dans le dataset *diamonds*, l’OrdinalEncoder est plus logique car les variables ont une hiérarchie naturelle.\n"
  },
  {
   "cell_type": "code",
   "id": "ed7b39e8-023b-485b-8f50-a3c127ad7a6f",
   "metadata": {
    "language": "python",
    "name": "cell34",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Encode categoricals to numeric columns\nsnowml_ohe = snowml.OneHotEncoder(input_cols=[\"CUT\", \"COLOR\", \"CLARITY\"], output_cols=[\"CUT_OHE\", \"COLOR_OHE\", \"CLARITY_OHE\"])\ntransformed_diamonds_df = snowml_ohe.fit(ord_encoded_diamonds_df).transform(ord_encoded_diamonds_df)\n\nnp.array(transformed_diamonds_df.columns)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1aa0f015-82b5-4d38-8fa1-99b8bd0b8969",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": "transformed_diamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a941d438-ceda-4857-85c0-0fa26e488357",
   "metadata": {
    "name": "cell36",
    "collapsed": false
   },
   "source": "Enfin, nous pouvons également construire un pipeline complet de prétraitement.\n\nCela sera utile à la fois pour les étapes d’entraînement et d’inférence du modèle ML, afin de disposer de transformations de variables standardisées.\n"
  },
  {
   "cell_type": "code",
   "id": "847e267b-3b18-433c-82f7-efd5ceaf2df9",
   "metadata": {
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": "# Categorize all the features for processing\nCATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\nCATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\nNUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n\ncategories = {\n    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n    \"COLOR\": np.array(['D', 'E', 'F', 'G', 'H', 'I', 'J']),\n}",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf88fe61-64a7-493b-a8d3-e6fdbde31ce1",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": "# Build the pipeline\npreprocessing_pipeline = Pipeline(\n    steps=[\n            (\n                \"OE\",\n                snowml.OrdinalEncoder(\n                    input_cols=CATEGORICAL_COLUMNS,\n                    output_cols=CATEGORICAL_COLUMNS_OE,\n                    categories=categories,\n                )\n            ),\n            (\n                \"MMS\",\n                snowml.MinMaxScaler(\n                    clip=True,\n                    input_cols=NUMERICAL_COLUMNS,\n                    output_cols=NUMERICAL_COLUMNS,\n                )\n            )\n    ]\n)\n\nPIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib'\njoblib.dump(preprocessing_pipeline, PIPELINE_FILE) # We are just pickling it locally first\n\ntransformed_diamonds_df = preprocessing_pipeline.fit(diamonds_df).transform(diamonds_df)\ntransformed_diamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "02116571-6d43-47ea-b56b-396036344165",
   "metadata": {
    "language": "sql",
    "name": "cell40"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE STAGE ML_HOL_ASSETS; --to store model assets",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac9b5da0-e28b-4c03-9da2-8aa365baf2cc",
   "metadata": {
    "language": "python",
    "name": "cell39"
   },
   "outputs": [],
   "source": "# You can also save the pickled object into the stage we created earlier for deployment\nsession.file.put(PIPELINE_FILE, \"@ML_HOL_ASSETS\", overwrite=True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "027b2aab-155b-4ef5-aa2a-5fb571c4e064",
   "metadata": {
    "name": "cell41",
    "collapsed": false
   },
   "source": "### Exploration des données\n\nMaintenant que nous avons transformé nos variables, calculons la corrélation entre chaque paire à l’aide de la fonction **correlation()** de Snowflake ML afin de mieux comprendre leurs relations.\n\nRemarque : la fonction de corrélation de Pearson de Snowflake ML renvoie un DataFrame Pandas.\n\n### À quoi sert l’étape de calcul de corrélation ?\n\nLe calcul de corrélation permet de **mesurer la relation linéaire entre deux variables numériques**.\n\nLa corrélation (souvent de Pearson) produit une valeur comprise entre **-1 et 1** :\n\n* **1** → corrélation positive forte (les variables évoluent dans le même sens)\n* **-1** → corrélation négative forte (elles évoluent en sens inverse)\n* **0** → absence de relation linéaire\n\n---\n\n### Objectifs dans un contexte Machine Learning\n\n#### 1. Comprendre les relations entre variables\n\nIdentifier quelles variables sont liées entre elles.\nExemple : le prix d’un diamant est souvent fortement corrélé au carat.\n\n#### 2. Détecter la multicolinéarité\n\nSi deux variables sont très fortement corrélées entre elles, elles apportent peut-être la même information.\nCela peut :\n\n* compliquer l’interprétation du modèle\n* dégrader certains modèles (ex. : régression linéaire)\n\n#### 3. Sélectionner les variables pertinentes\n\nUne variable fortement corrélée à la variable cible peut être un bon prédicteur.\n\n#### 4. Vérifier la cohérence des transformations\n\nAprès le preprocessing, on peut s’assurer que les relations attendues sont toujours présentes.\n\n---\n\n### Résumé\n\nL’étape de calcul de corrélation sert à **analyser les dépendances entre variables**, améliorer la compréhension des données et préparer une sélection de variables plus pertinente avant l’entraînement du modèle.\n\n"
  },
  {
   "cell_type": "code",
   "id": "828e7c07-a67e-4034-8015-69b192fbf954",
   "metadata": {
    "language": "python",
    "name": "cell42"
   },
   "outputs": [],
   "source": "corr_diamonds_df = correlation(df=transformed_diamonds_df)\ncorr_diamonds_df # This is a Pandas DataFrame",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7a69e18-a84a-4b02-85b7-ba5a666f26bf",
   "metadata": {
    "language": "python",
    "name": "cell43"
   },
   "outputs": [],
   "source": "# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr_diamonds_df, dtype=bool))\n\n# Create a heatmap with the features\nplt.figure(figsize=(7, 7))\nheatmap = sns.heatmap(corr_diamonds_df, mask=mask, cmap=\"YlGnBu\", annot=True, vmin=-1, vmax=1)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "74dd55bb-9215-4680-8a07-58b83ebf303e",
   "metadata": {
    "name": "cell44",
    "collapsed": false
   },
   "source": "Nous constatons que **CARAT** et **PRICE** sont fortement corrélés, ce qui est logique. Examinons leur relation d’un peu plus près.\n\nRemarque : vous devrez convertir votre DataFrame Snowpark en DataFrame Pandas afin d’utiliser **matplotlib** et **seaborn**.\n"
  },
  {
   "cell_type": "code",
   "id": "92ae551f-175d-4539-be71-2958678f0079",
   "metadata": {
    "language": "python",
    "name": "cell45"
   },
   "outputs": [],
   "source": "# Set up a plot to look at CARAT and PRICE\ncounts = transformed_diamonds_df.to_pandas().groupby(['PRICE', 'CARAT', 'CLARITY_OE']).size().reset_index(name='Count')\n\nfig, ax = plt.subplots(figsize=(20, 20))\nplt.title('Price vs Carat', fontsize=28)\nax = sns.scatterplot(data=counts, x='CARAT', y='PRICE', size='Count', hue='CLARITY_OE', markers='o')\nax.grid(axis='y')\n\n# The relationship is not linear - it appears exponential which makes sense given the rarity of the large diamonds\nsns.move_legend(ax, \"upper left\")\nsns.despine(left=True, bottom=True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8134ba7b-26df-4ae6-bdc3-57fc103dafaa",
   "metadata": {
    "name": "cell46",
    "collapsed": false
   },
   "source": "Maintenant, Nous allons voir comment entraîner un modèle XGBoost avec le jeu de données diamonds."
  },
  {
   "cell_type": "markdown",
   "id": "093fde98-bcad-452c-bc6c-554f56ca2ddc",
   "metadata": {
    "name": "cell47",
    "collapsed": false
   },
   "source": "### 3. Modélisation ML\n\nDans ce notebook, nous allons illustrer comment entraîner un modèle **XGBoost** avec le jeu de données *diamonds* en utilisant **XGBoost open source (OSS)**.\nNous montrerons également comment effectuer l’inférence et gérer les modèles via le **Model Registry**.\n\n\n### Import Libraries"
  },
  {
   "cell_type": "code",
   "id": "18178805-747d-43ac-b6d0-61cd20a60e7c",
   "metadata": {
    "language": "sql",
    "name": "cell49"
   },
   "outputs": [],
   "source": "USE DATABASE ML_LAB_DB;\n\nUSE SCHEMA ML_SCHEMA;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2809f09f-3525-4cdc-9b1d-efa1acf93385",
   "metadata": {
    "language": "python",
    "name": "cell48"
   },
   "outputs": [],
   "source": "# Get Snowflake Session object\nsession = get_active_session()\nsession.sql_simplifier_enabled = True\n\n# Add a query tag to the session.\nsession.query_tag = {\"origin\":\"Axel_T\", \n                     \"name\":\"Ml_diamands\", \n                     \"version\":{\"major\":1, \"minor\":0,},\n                     \"attributes\":{\"is_quickstart\":1}}\n\n# Current Environment Details\nprint('Connection Established with the following parameters:')\nprint('User      : {}'.format(session.get_current_user()))\nprint('Role      : {}'.format(session.get_current_role()))\nprint('Database  : {}'.format(session.get_current_database()))\nprint('Schema    : {}'.format(session.get_current_schema()))\nprint('Warehouse : {}'.format(session.get_current_warehouse()))\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a141598-ffc2-4262-b243-01074fa478e3",
   "metadata": {
    "language": "python",
    "name": "cell50"
   },
   "outputs": [],
   "source": "# Snowpark for Python\nfrom snowflake.snowpark.version import VERSION\nimport snowflake.snowpark.functions as F\n\n# Snowflake ML\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml._internal.utils import identifier\n\n# data science libs\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBRegressor #importer une version 2.1.3\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.model_selection import GridSearchCV\nimport shape # impoter une version 0.48.0\n\n# misc\nimport json\nimport joblib\nimport cachetools\n\n# warning suppresion\nimport warnings; warnings.simplefilter('ignore')\n\nprint(\"shap:\", shap.__version__)\nprint(\"xgboost:\", xgboost.__version__)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "224cf6f9-b0a2-463c-8528-a242cac44dcc",
   "metadata": {
    "name": "cell51",
    "collapsed": false
   },
   "source": "### Charger les données et le pipeline de prétraitement."
  },
  {
   "cell_type": "code",
   "id": "502454db-e248-464a-b48d-44e327256cd8",
   "metadata": {
    "language": "python",
    "name": "cell52"
   },
   "outputs": [],
   "source": "# Load in the data\ndiamonds_df = session.table(\"DIAMONDS\")\ndiamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a27e3fc6-913e-449a-a617-f477c7ff52b7",
   "metadata": {
    "language": "python",
    "name": "cell53"
   },
   "outputs": [],
   "source": "# Categorize all the features for modeling\nCATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\nCATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\nNUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n\nLABEL_COLUMNS = ['PRICE']\nOUTPUT_COLUMNS = ['PREDICTED_PRICE']",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79819388-51f8-420c-bd3d-e1f7b93339d0",
   "metadata": {
    "language": "python",
    "name": "cell54"
   },
   "outputs": [],
   "source": "# Load the preprocessing pipeline object from stage- to do this, we download the preprocessing_pipeline.joblib.gz file to the warehouse\n# where our notebook is running, and then load it using joblib.\nsession.file.get('@ML_HOL_ASSETS/preprocessing_pipeline.joblib.gz', '/tmp')\nPIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib.gz'\npreprocessing_pipeline = joblib.load(PIPELINE_FILE)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b28baefe-cc05-43ed-b4e3-7c3aff690503",
   "metadata": {
    "name": "cell55",
    "collapsed": false
   },
   "source": "### Construire un modèle de régression XGBoost open source simple."
  },
  {
   "cell_type": "code",
   "id": "7a140f1e-d021-43e6-aad7-23dd18279a1f",
   "metadata": {
    "language": "python",
    "name": "cell57"
   },
   "outputs": [],
   "source": "# Split the data into train and test sets\ndiamonds_train_df, diamonds_test_df = diamonds_df.random_split(weights=[0.9, 0.1], seed=0)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d59b47b6-69a9-4f84-9f83-29d8bb31d065",
   "metadata": {
    "language": "python",
    "name": "cell58"
   },
   "outputs": [],
   "source": "diamonds_train_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7693cf60-de08-433e-ad51-04f365abf114",
   "metadata": {
    "language": "python",
    "name": "cell59"
   },
   "outputs": [],
   "source": "diamonds_test_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b00ad53-df11-4df7-8009-f358a426476d",
   "metadata": {
    "language": "python",
    "name": "cell56"
   },
   "outputs": [],
   "source": "# Run the train and test sets through the Pipeline object we defined earlier\ntrain_df = preprocessing_pipeline.fit(diamonds_train_df).transform(diamonds_train_df)\ntest_df = preprocessing_pipeline.transform(diamonds_test_df)\n\n# Convert to pandas dataframes to use OSS XGBoost\ntrain_pd = train_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS+LABEL_COLUMNS).to_pandas()\ntest_pd = test_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS+LABEL_COLUMNS).to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83f21b27-325e-4b37-9f9e-088b5f817da2",
   "metadata": {
    "language": "python",
    "name": "cell60"
   },
   "outputs": [],
   "source": "train_pd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf612b7e-fb65-4ecf-892e-03bcbbbd1b6c",
   "metadata": {
    "language": "python",
    "name": "cell61"
   },
   "outputs": [],
   "source": "test_pd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17b0195e-b3fc-4f6b-a8dd-d6ea29fb8a18",
   "metadata": {
    "language": "python",
    "name": "cell62"
   },
   "outputs": [],
   "source": "# Define model config\nregressor = XGBRegressor()\n\n# Split train data into X, y\ny_train_pd = train_pd.PRICE\nX_train_pd = train_pd.drop(columns=['PRICE'])\n\n# Train model\nregressor.fit(X_train_pd, y_train_pd)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "023ff116-44cc-4829-9efb-e73f296edc81",
   "metadata": {
    "language": "python",
    "name": "cell63"
   },
   "outputs": [],
   "source": "# We can now get predictions\ny_test_pred = regressor.predict(test_pd.drop(columns=['PRICE']))\ny_train_pred = regressor.predict(train_pd.drop(columns=['PRICE']))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa352b3f-9367-4046-9050-bb4ed40fd3d5",
   "metadata": {
    "language": "python",
    "name": "cell64"
   },
   "outputs": [],
   "source": "y_test_pred",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44353789-5c7b-4425-b51a-2678fdb0a020",
   "metadata": {
    "language": "python",
    "name": "cell65"
   },
   "outputs": [],
   "source": "y_train_pred",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c3ca60c9-6f6e-4842-b533-0fe4efb73e06",
   "metadata": {
    "name": "cell66",
    "collapsed": false
   },
   "source": "Analysons les résultats à l’aide du **MAPE** de Snowflake ML.\n\n### Explication de l’étape : analyser les résultats avec le MAPE\n\nLe **MAPE** (Mean Absolute Percentage Error — erreur absolue moyenne en pourcentage) est une métrique utilisée pour évaluer la performance d’un **modèle de régression**.\n\nIl mesure l’écart moyen entre les valeurs réelles et les valeurs prédites, exprimé en **pourcentage**.\n\n---\n\n### Que signifie le résultat ?\n\n* **MAPE faible** → bonnes prédictions\n* **MAPE élevé** → erreurs importantes\n\nExemple :\nUn MAPE de **5 %** signifie que, en moyenne, les prédictions s’écartent de 5 % des valeurs réelles.\n\n---\n\n### Pourquoi utiliser le MAPE ?\n\n1. **Interprétation intuitive** : exprimé en pourcentage\n2. **Comparaison facile** entre modèles\n3. Adapté aux problèmes de prédiction de valeurs positives (comme le prix des diamants)\n\n---\n\n### Dans ce contexte\n\nAprès avoir entraîné le modèle XGBoost pour prédire le **PRICE**, on calcule le MAPE pour mesurer :\n\n* la précision globale du modèle\n* sa capacité à prédire correctement le prix des diamants\n\n---\n\n### Résumé\n\nCette étape consiste à **évaluer la qualité du modèle** en mesurant l’erreur moyenne en pourcentage entre les prix prédits et les prix réels.\n\n"
  },
  {
   "cell_type": "code",
   "id": "5fa05952-d93b-4471-8db5-ba77747261b1",
   "metadata": {
    "language": "python",
    "name": "cell67"
   },
   "outputs": [],
   "source": "mape = mean_absolute_percentage_error(y_train_pd, y_train_pred)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c1a3967-1b6a-462b-929c-72fb676245b1",
   "metadata": {
    "language": "python",
    "name": "cell68"
   },
   "outputs": [],
   "source": "print(f\"Mean absolute percentage error: {mape * 100} %\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "83880c6a-b757-4556-9a4f-4ba27ccd1ccc",
   "metadata": {
    "name": "cell69",
    "collapsed": false
   },
   "source": "Maintenant, utilisons la fonction **GridSearchCV** de **scikit-learn** pour trouver les paramètres optimaux du modèle."
  },
  {
   "cell_type": "code",
   "id": "3ba058f6-129f-4245-b96a-b10eebf741ce",
   "metadata": {
    "language": "python",
    "name": "cell70"
   },
   "outputs": [],
   "source": "parameters={\n        \"n_estimators\":[100, 200, 500],\n        \"learning_rate\":[0.1, 0.4]\n}\n\nxgb = XGBRegressor()\nclf = GridSearchCV(xgb, parameters)\nclf.fit(X_train_pd, y_train_pd)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6817fd69-2446-4a2d-9805-55c01a0c6a37",
   "metadata": {
    "name": "cell71",
    "collapsed": false
   },
   "source": "### Explication du code\n\nCe code réalise une **recherche des meilleurs hyperparamètres** pour un modèle **XGBoost Regressor** à l’aide de **GridSearchCV**.\n\n---\n\n### 1. Définition des hyperparamètres à tester\n\n```python\nparameters={\n    \"n_estimators\":[100, 200, 500],\n    \"learning_rate\":[0.1, 0.4]\n}\n```\n\nOn définit une grille de combinaisons :\n\n* **n_estimators** : nombre d’arbres (100, 200 ou 500)\n* **learning_rate** : taux d’apprentissage (0.1 ou 0.4)\n\nCela crée **6 combinaisons possibles** (3 × 2).\n\n---\n\n### 2. Création du modèle\n\n```python\nxgb = XGBRegressor()\n```\n\nOn initialise un modèle de régression XGBoost.\n\n---\n\n### 3. Recherche des meilleurs paramètres\n\n```python\nclf = GridSearchCV(xgb, parameters)\n```\n\nGridSearchCV :\n\n* teste toutes les combinaisons définies\n* évalue chaque modèle via validation croisée\n* compare leurs performances\n\n---\n\n### 4. Entraînement\n\n```python\nclf.fit(X_train_pd, y_train_pd)\n```\n\nLe modèle est entraîné sur les données d’entraînement.\nGridSearchCV :\n\n* entraîne plusieurs modèles\n* calcule leur score\n* sélectionne automatiquement la meilleure combinaison d’hyperparamètres\n\n---\n\n### Résumé\n\nCe code teste plusieurs configurations de XGBoost et sélectionne automatiquement celle qui donne les meilleures performances sur les données d’entraînement.\n"
  },
  {
   "cell_type": "code",
   "id": "ed760621-b022-484d-8558-d8879abbfc7c",
   "metadata": {
    "language": "python",
    "name": "cell72"
   },
   "outputs": [],
   "source": "print(clf.best_estimator_)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ee3cea5-b04c-4539-81e9-33a3ec1efcdf",
   "metadata": {
    "name": "cell73",
    "collapsed": false
   },
   "source": "Nous constatons que le meilleur estimateur possède les paramètres suivants : **n_estimators = 500** et **learning_rate = 0.4**.\n\n```\nXGBRegressor(base_score=None, booster=None, callbacks=None,  \n             colsample_bylevel=None, colsample_bynode=None,  \n             colsample_bytree=None, device=None, early_stopping_rounds=None,  \n             enable_categorical=False, eval_metric=None, feature_types=None,  \n             feature_weights=None, gamma=None, grow_policy=None,  \n             importance_type=None, interaction_constraints=None,  \n             **learning_rate=0.4**, max_bin=None, max_cat_threshold=None,  \n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,  \n             max_leaves=None, min_child_weight=None, missing=nan,  \n             monotone_constraints=None, multi_strategy=None, **n_estimators=500**,  \n             n_jobs=None, num_parallel_tree=None, ...)\n\n```\n\nNous pouvons également analyser l’ensemble des résultats de la recherche par grille (grid search).\n"
  },
  {
   "cell_type": "code",
   "id": "8360ef53-3b02-4a06-a8da-2d6a77d41d8f",
   "metadata": {
    "language": "python",
    "name": "cell74"
   },
   "outputs": [],
   "source": "# Analyze grid search results\ngs_results = clf.cv_results_\nn_estimators_val = []\nlearning_rate_val = []\nfor param_dict in gs_results[\"params\"]:\n    n_estimators_val.append(param_dict[\"n_estimators\"])\n    learning_rate_val.append(param_dict[\"learning_rate\"])\nmape_val = gs_results[\"mean_test_score\"]*-1\n\ngs_results_df = pd.DataFrame(data={\n    \"n_estimators\":n_estimators_val,\n    \"learning_rate\":learning_rate_val,\n    \"mape\":mape_val})\n\nsns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")\n\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c5cfa428-6fdf-4863-af4e-2a35ee16ec0c",
   "metadata": {
    "name": "cell75",
    "collapsed": false
   },
   "source": "Cela est cohérent avec **learning_rate = 0,4** et **n_estimators = 500**, sélectionnés comme le meilleur estimateur avec le **MAPE le plus faible**.\n\nMaintenant, effectuons des prédictions et analysons les résultats obtenus à partir du meilleur estimateur.\n"
  },
  {
   "cell_type": "code",
   "id": "e22cc39e-0c5f-4f64-8355-5f0c173bff09",
   "metadata": {
    "language": "python",
    "name": "cell76"
   },
   "outputs": [],
   "source": "from sklearn.metrics import mean_absolute_percentage_error\n\n# Predict\nopt_model = clf.best_estimator_\ny_train_pred = opt_model.predict(train_pd.drop(columns=['PRICE']))\n\nmape = mean_absolute_percentage_error(y_train_pd, y_train_pred)\n\nprint(f\"Mean absolute percentage error: {mape}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "665d1249-f3aa-4e04-8797-1eac8aecb990",
   "metadata": {
    "name": "cell77",
    "collapsed": false
   },
   "source": "Enregistrons notre modèle optimal ainsi que ses métadonnées :"
  },
  {
   "cell_type": "code",
   "id": "9cbae7c9-62ef-42d9-b6f3-ef374eef17a9",
   "metadata": {
    "language": "python",
    "name": "cell78"
   },
   "outputs": [],
   "source": "optimal_model = clf.best_estimator_\noptimal_n_estimators = clf.best_estimator_.n_estimators\noptimal_learning_rate = clf.best_estimator_.learning_rate\n\noptimal_mape = gs_results_df.loc[(gs_results_df['n_estimators']==optimal_n_estimators) &\n                                 (gs_results_df['learning_rate']==optimal_learning_rate), 'mape'].values[0]",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa71f069-9eb0-439d-8c21-1cffe4a33ea0",
   "metadata": {
    "name": "cell79",
    "collapsed": false
   },
   "source": "### Gérer les modèles à l’aide du Model Registry\n\nAvec le Model Registry de Snowflake ML, nous disposons d’un framework natif Snowflake pour le versioning et le déploiement des modèles. Cela nous permet d’enregistrer les modèles, d’étiqueter les paramètres et les métriques, de suivre les métadonnées, de créer des versions et, en fin de compte, d’exécuter des tâches d’inférence par lots dans un entrepôt Snowflake ou de déployer vers un Snowpark Container Service.\n\nNous allons d’abord enregistrer nos modèles.\n"
  },
  {
   "cell_type": "code",
   "id": "126f5d9e-67eb-4190-b61c-258631c34e2e",
   "metadata": {
    "language": "python",
    "name": "cell80"
   },
   "outputs": [],
   "source": "# Get sample input data to pass into the registry logging function\nX = train_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS).limit(100)\n\ndb = identifier._get_unescaped_name(session.get_current_database())\nschema = identifier._get_unescaped_name(session.get_current_schema())\n\n# Define model name\nmodel_name = \"DIAMONDS_PRICE_PREDICTION\"\n\n# Create a registry and log the model\nnative_registry = Registry(session=session, database_name=db, schema_name=schema)\n\n# Let's first log the very first model we trained\nmodel_ver = native_registry.log_model(\n    model_name=model_name,\n    version_name='V0',\n    model=regressor,\n    sample_input_data=X, # to provide the feature schema\n    target_platforms={'WAREHOUSE'}\n)\n\n# Add evaluation metric\nmodel_ver.set_metric(metric_name=\"mean_abs_pct_err\", value=mape)\n\n# Add a description\nmodel_ver.comment = \"This is the first iteration of our Diamonds Price Prediction model. It is used for demo purposes.\"\n\n# Now, let's log the optimal model from GridSearchCV\nmodel_ver2 = native_registry.log_model(\n    model_name=model_name,\n    version_name='V1',\n    model=optimal_model,\n    sample_input_data=X, # to provide the feature schema\n    target_platforms={'WAREHOUSE'}\n)\n\n# Add evaluation metric\nmodel_ver2.set_metric(metric_name=\"mean_abs_pct_err\", value=optimal_mape)\n\n# Add a description\nmodel_ver2.comment = f\"This is the second iteration of our Diamonds Price Prediction model \\\n                        where we performed hyperparameter optimization. \\\n                        Optimal n_estimators & learning_rate: {optimal_n_estimators}, {optimal_learning_rate}\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "630456e3-4756-48fa-ad5b-8466f4036f15",
   "metadata": {
    "language": "python",
    "name": "cell81"
   },
   "outputs": [],
   "source": "# Let's confirm they were added\nnative_registry.get_model(model_name).show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cad61fa7-8dc1-4905-9b78-bdeafd4293af",
   "metadata": {
    "name": "cell82",
    "collapsed": false
   },
   "source": "Nous pouvons voir quel est le modèle par défaut lorsque nous avons plusieurs versions portant le même nom de modèle :\n"
  },
  {
   "cell_type": "code",
   "id": "5b3fe34d-e201-42b9-907f-11bd08833847",
   "metadata": {
    "language": "python",
    "name": "cell83"
   },
   "outputs": [],
   "source": "native_registry.get_model(model_name).default.version_name",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d28f041-06e7-4c8f-994f-5465649629ad",
   "metadata": {
    "name": "cell84",
    "collapsed": false
   },
   "source": "Nous pouvons maintenant utiliser le modèle optimal pour effectuer l’inférence.\n"
  },
  {
   "cell_type": "code",
   "id": "9c595e12-5450-472f-81ab-27f11b4fe9eb",
   "metadata": {
    "language": "python",
    "name": "cell85"
   },
   "outputs": [],
   "source": "model_ver = native_registry.get_model(model_name).version('v1')\nresult_sdf2 = model_ver.run(test_df, function_name=\"predict\")\nresult_sdf2.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0737a6e-451a-4ef4-ba18-46b076c171b0",
   "metadata": {
    "name": "cell86",
    "collapsed": false
   },
   "source": "Vous pouvez également exécuter l’inférence à l’aide de SQL. Pour cela, nous utiliserons une cellule SQL et appellerons la méthode **predict** du modèle en référant le nom de l’objet modèle.\n"
  },
  {
   "cell_type": "code",
   "id": "073f7d5a-7f69-4411-9505-c7355d48e3ae",
   "metadata": {
    "language": "python",
    "name": "cell87"
   },
   "outputs": [],
   "source": "test_df.write.mode('overwrite').save_as_table('DIAMONDS_TEST')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c9874d6d-dcc1-41cc-82b3-1f0de91987b9",
   "metadata": {
    "language": "sql",
    "name": "cell88"
   },
   "outputs": [],
   "source": "--- for any other version (for example V1 below):\nWITH model_version_alias AS MODEL DIAMONDS_PRICE_PREDICTION VERSION v1 SELECT \na.*, \nmodel_version_alias!predict(\n    a.CUT_OE,\n    a.COLOR_OE, \n    a.CLARITY_OE, \n    a.CARAT, \n    a.DEPTH, \n    a.TABLE_PCT, \n    a.X, \n    a.Y, \n    a.Z\n)['output_feature_0'] as prediction \nfrom DIAMONDS_TEST a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84c9b6a0-33f7-4779-a6d3-5e15bf02f73b",
   "metadata": {
    "name": "cell89",
    "collapsed": false
   },
   "source": "### Explicabilité du modèle\n\nUn autre aspect que nous pouvons examiner pour mieux comprendre les prédictions consiste à analyser les explications sur ce que le modèle considère comme le plus déterminant lors de la génération des prédictions. Pour produire ces explications, nous utiliserons la fonction d’explicabilité intégrée de Snowflake ML.\n\nEn interne, cette fonction repose sur les **valeurs de Shapley**. Lors du processus d’entraînement, les modèles de machine learning apprennent des relations entre les entrées et les sorties, et les valeurs de Shapley permettent d’attribuer la prédiction d’un modèle à ses variables d’entrée. En considérant toutes les combinaisons possibles de variables, les valeurs de Shapley mesurent la contribution marginale moyenne de chaque variable à la prédiction du modèle. Bien que cette méthode soit coûteuse en calcul, les informations obtenues sont précieuses pour l’interprétabilité et le débogage du modèle.\n\nCalculons maintenant ces explications à partir de notre modèle optimal.\n"
  },
  {
   "cell_type": "code",
   "id": "104efe7d-aacb-4cd6-ad7f-6ee35f7bf47d",
   "metadata": {
    "language": "python",
    "name": "cell90"
   },
   "outputs": [],
   "source": "mv_explanations = model_ver.run(train_df, function_name=\"explain\")\nmv_explanations",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1dac3cd4-145b-49c0-b603-5ca2077412dd",
   "metadata": {
    "name": "cell91",
    "collapsed": false
   },
   "source": "Visualisons ces explications, car il est un peu difficile d’interpréter directement les valeurs telles quelles."
  },
  {
   "cell_type": "code",
   "id": "9c1cde23-1145-46b0-a351-727b5dba9fba",
   "metadata": {
    "language": "python",
    "name": "cell92"
   },
   "outputs": [],
   "source": "import shap\n\n# Create a sample of 1000 records\ntest_pd = test_df.to_pandas()\ntest_pd_sample = test_pd.sample(n=1000, random_state = 100).reset_index(drop=True)\n\n# Compute shapley values for each model\nshap_pd = model_ver.run(test_pd_sample, function_name=\"explain\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2147baf-2920-44a8-8c03-5a8bee2ca781",
   "metadata": {
    "name": "cell93",
    "collapsed": false
   },
   "source": "Nous constatons que **CARAT** a l’impact le plus important sur les valeurs prédites (**PRICE**), suivi de la dimension **Y**, de **CLARITY** et de **COLOR**. Cela correspond à ce que nous avions observé lors de la phase d’exploration des données dans le notebook précédent, notamment lors du graphique **PRICE vs CARAT**.\n\nEnregistrons maintenant nos données d’entraînement dans une table Snowflake afin d’illustrer comment la version SQL de cette fonction peut également être utilisée pour générer des explications des variables.\n"
  },
  {
   "cell_type": "code",
   "id": "9ce994e2-f3ab-44f2-88b3-fa061667d61c",
   "metadata": {
    "language": "python",
    "name": "cell94"
   },
   "outputs": [],
   "source": "train_df.write.mode('overwrite').save_as_table('DIAMONDS_TRAIN')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "020b5fde-d0cf-46b7-8906-b71cd40b5cc6",
   "metadata": {
    "name": "cell95",
    "collapsed": false
   },
   "source": "Nous pouvons maintenant appeler l’API SQL en invoquant le modèle ainsi que la version que nous souhaitons évaluer afin de générer ces explications.\n"
  },
  {
   "cell_type": "code",
   "id": "9ed727f7-26c1-427a-9a64-9dac0d58b7f8",
   "metadata": {
    "language": "sql",
    "name": "cell96"
   },
   "outputs": [],
   "source": "WITH mv AS MODEL \"DIAMONDS_PRICE_PREDICTION\" VERSION \"V1\"\nSELECT * FROM DIAMONDS_TRAIN,\n  TABLE(mv!\"EXPLAIN\"(\n    CUT_OE,\n    COLOR_OE,\n    CLARITY_OE,\n    CARAT,\n    DEPTH,\n    TABLE_PCT,\n    X,\n    Y,\n    Z\n  ));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "840575f8-b6e4-490b-bfa9-6b31ffb9aeca",
   "metadata": {
    "name": "cell97",
    "collapsed": false
   },
   "source": "Pour fininr, nouas allons créerune application streamlit:"
  },
  {
   "cell_type": "code",
   "id": "fc1b7b43-14c4-49fc-91e7-1b7f004a5456",
   "metadata": {
    "language": "python",
    "name": "cell98"
   },
   "outputs": [],
   "source": "import streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import col, avg, min, max\nimport pandas as pd\nimport altair as alt\nimport joblib\nimport os\n\n# --- App Title and Description ---\nst.title(\"Diamond Price Prediction\")\nst.write(\n    \"This application predicts the price of a diamond based on its characteristics. \"\n    \"Use the sliders and dropdowns below to enter the diamond's features and click 'Predict Price' to see the estimated value.\"\n)\n\n# --- Snowflake Session ---\nsession = get_active_session()\n\n# --- Database and Schema Information ---\n# This assumes the user has run the setup.sql and notebooks from the repository\nDB_NAME = 'ML_LAB_DB'\nSCHEMA_NAME = 'ML_SCHEMA'\nSTAGE_NAME = 'ML_HOL_ASSETS'\n# Using the raw training data table as it contains the unprocessed features\nTRAINING_TABLE = 'DIAMONDS_TRAIN' \nMODEL_NAME = 'DIAMONDS_PRICE_PREDICTION'\nMODEL_VERSION = 'v1'\nPIPELINE_FILE_NAME = 'preprocessing_pipeline.joblib.gz'\nLOCAL_PIPELINE_PATH = f'/tmp/{PIPELINE_FILE_NAME}'\n\n# --- Load Preprocessing Pipeline ---\n# This function downloads the pipeline from stage and caches it for reuse.\n@st.cache_resource\ndef load_pipeline():\n    \"\"\"Downloads the preprocessing pipeline file from stage and loads it into memory.\"\"\"\n    try:\n        session.file.get(f'@{STAGE_NAME}/{PIPELINE_FILE_NAME}', '/tmp')\n        pipeline = joblib.load(LOCAL_PIPELINE_PATH)\n        return pipeline\n    except Exception as e:\n        st.error(f\"Failed to load preprocessing pipeline from stage '@{STAGE_NAME}'.\")\n        st.error(f\"Please ensure '{PIPELINE_FILE_NAME}' exists in the stage. Error: {e}\")\n        return None\n\ntry:\n    with st.spinner(\"Loading preprocessing pipeline...\"):\n        preprocessing_pipeline = load_pipeline()\n    if preprocessing_pipeline:\n        st.success(\"Preprocessing pipeline loaded successfully!\")\n    else:\n        st.stop()\nexcept Exception as e:\n    st.error(f\"An error occurred while loading the pipeline: {e}\")\n    st.stop()\n\n\n# --- Feature Input from User ---\nst.sidebar.header(\"Diamond Features\")\n\n# Define the logical order for categorical features to be used in UI and for encoding\nCATEGORICAL_ORDER = {\n    \"CUT\": [\"FAIR\", \"GOOD\", \"VERY_GOOD\", \"PREMIUM\", \"IDEAL\"],\n    \"COLOR\": [\"J\", \"I\", \"H\", \"G\", \"F\", \"E\", \"D\"],\n    \"CLARITY\": [\"I1\", \"SI2\", \"SI1\", \"VS2\", \"VS1\", \"VVS2\", \"VVS1\", \"IF\"]\n}\n\ndef get_feature_ranges(table_name):\n    \"\"\"Gets the min and max for numerical columns and distinct values for categorical columns.\"\"\"\n    # User should input raw features; the pipeline will create derived features like TABLE_PCT\n    numerical_features = ['CARAT', 'DEPTH', 'TABLE_PCT', 'X', 'Y', 'Z']\n    categorical_features = ['CUT', 'COLOR', 'CLARITY']\n\n    feature_ranges = {}\n    df = session.table(table_name)\n\n    # Get min/max for numerical features\n    for feature in numerical_features:\n        result_row = df.agg(min(col(feature)).alias(\"MIN_VAL\"), max(col(feature)).alias(\"MAX_VAL\")).collect()[0]\n        min_val = result_row[\"MIN_VAL\"]\n        max_val = result_row[\"MAX_VAL\"]\n        feature_ranges[feature] = (float(min_val), float(max_val))\n\n    # Get distinct values for categorical features and sort them logically\n    for feature in categorical_features:\n        feature_ranges[feature] = CATEGORICAL_ORDER[feature]\n\n    return feature_ranges\n\ntry:\n    with st.spinner(\"Loading feature ranges from Snowflake...\"):\n        feature_ranges = get_feature_ranges(TRAINING_TABLE)\n\n    # Create input widgets for raw features\n    carat = st.sidebar.slider(\"Carat\", feature_ranges['CARAT'][0], feature_ranges['CARAT'][1], float(feature_ranges['CARAT'][0] + feature_ranges['CARAT'][1]) / 2)\n    depth = st.sidebar.slider(\"Depth\", feature_ranges['DEPTH'][0], feature_ranges['DEPTH'][1], float(feature_ranges['DEPTH'][0] + feature_ranges['DEPTH'][1]) / 2)\n    table = st.sidebar.slider(\"TABLE_PCT\", feature_ranges['TABLE_PCT'][0], feature_ranges['TABLE_PCT'][1], float(feature_ranges['TABLE_PCT'][0] + feature_ranges['TABLE_PCT'][1]) / 2)\n    x = st.sidebar.slider(\"X (Length in mm)\", feature_ranges['X'][0], feature_ranges['X'][1], float(feature_ranges['X'][0] + feature_ranges['X'][1]) / 2)\n    y = st.sidebar.slider(\"Y (Width in mm)\", feature_ranges['Y'][0], feature_ranges['Y'][1], float(feature_ranges['Y'][0] + feature_ranges['Y'][1]) / 2)\n    z = st.sidebar.slider(\"Z (Depth in mm)\", feature_ranges['Z'][0], feature_ranges['Z'][1], float(feature_ranges['Z'][0] + feature_ranges['Z'][1]) / 2)\n\n    cut = st.sidebar.selectbox(\"Cut\", feature_ranges['CUT'])\n    color = st.sidebar.selectbox(\"Color\", feature_ranges['COLOR'])\n    clarity = st.sidebar.selectbox(\"Clarity\", feature_ranges['CLARITY'])\n\nexcept Exception as e:\n    st.error(f\"Could not load feature ranges. Make sure the table '{TRAINING_TABLE}' exists in '{DB_NAME}.{SCHEMA_NAME}'. Error: {e}\")\n    st.stop()\n\n\n# --- Prediction ---\nif st.sidebar.button(\"Predict Price\", type=\"primary\"):\n    with st.spinner(\"Preprocessing input and predicting price...\"):\n        try:\n            # --- FEATURE ENGINEERING VIA PIPELINE ---\n            # 1. Create a pandas DataFrame from user inputs with the raw feature names\n            raw_features_df = pd.DataFrame(\n                [[carat, cut, color, clarity, depth, table, x, y, z]],\n                columns=['CARAT', 'CUT', 'COLOR', 'CLARITY', 'DEPTH', 'TABLE_PCT', 'X', 'Y', 'Z']\n            )\n\n            # 2. Run the raw features through the loaded preprocessing pipeline\n            processed_features_df = preprocessing_pipeline.fit(raw_features_df).transform(raw_features_df)\n            \n            # 3. Get the processed feature values for the SQL query\n            sql_column_order = ['CUT_OE', 'COLOR_OE', 'CLARITY_OE', 'CARAT', 'DEPTH', 'TABLE_PCT', 'X', 'Y', 'Z']\n            feature_values = processed_features_df[sql_column_order].iloc[0].values.tolist()\n            values_str = ', '.join(map(str, feature_values))\n            \n            # --- PREDICTION ---\n            # Construct the SQL to call the prediction model from the registry\n            # The model expects features in the order output by the pipeline\n            prediction_sql = f\"\"\"\n                WITH model_version_alias AS MODEL {MODEL_NAME} VERSION {MODEL_VERSION}\n                SELECT model_version_alias!predict(\n                    t.CUT_OE, t.COLOR_OE, t.CLARITY_OE, t.CARAT, t.DEPTH, t.TABLE_PCT, t.X, t.Y, t.Z\n                )['output_feature_0'] as PREDICTION\n                FROM (\n                    VALUES ({values_str})\n                ) AS t(CUT_OE, COLOR_OE, CLARITY_OE, CARAT, DEPTH, TABLE_PCT, X, Y, Z)\n            \"\"\"\n\n            result_df = session.sql(prediction_sql).collect()\n            predicted_price = result_df[0]['PREDICTION']\n\n            st.metric(\"Predicted Diamond Price\", f\"${float(predicted_price):,.2f}\")\n\n            # --- Visualizations ---\n            st.subheader(\"Feature Comparison\")\n\n            # Get average values from the training data for visualization\n            training_df_vis = session.table(TRAINING_TABLE)\n            # FIX: The column name 'TABLE' is a reserved SQL keyword and must be quoted.\n            avg_values = training_df_vis.select(\n                avg(\"CARAT\").alias(\"Avg Carat\"),\n                avg(\"DEPTH\").alias(\"Avg Depth\"),\n                avg(col(\"TABLE_PCT\")).alias(\"Avg Table\"),\n                avg(\"X\").alias(\"Avg X\"),\n                avg(\"Y\").alias(\"Avg Y\"),\n                avg(\"Z\").alias(\"Avg Z\")\n            ).to_pandas()\n\n            # Prepare data for charting using raw user inputs for interpretability\n            user_input_data = {\n                'Feature': ['Carat', 'Depth', 'Table_PCT', 'X', 'Y', 'Z'],\n                'Value': [carat, depth, table, x, y, z],\n                'Source': 'Your Input'\n            }\n            avg_data = {\n                'Feature': ['Carat', 'Depth', 'Table_PCT', 'X', 'Y', 'Z'],\n                'Value': [\n                    avg_values['Avg Carat'][0],\n                    avg_values['Avg Depth'][0],\n                    avg_values['Avg Table'][0],\n                    avg_values['Avg X'][0],\n                    avg_values['Avg Y'][0],\n                    avg_values['Avg Z'][0]\n                ],\n                'Source': 'Training Set Average'\n            }\n\n            chart_data = pd.concat([pd.DataFrame(user_input_data), pd.DataFrame(avg_data)])\n\n            # Create the chart\n            chart = alt.Chart(chart_data).mark_bar().encode(\n                x=alt.X('Feature:N', sort=None),\n                y=alt.Y('Value:Q'),\n                color='Source:N',\n                tooltip=['Feature', 'Value', 'Source']\n            ).properties(\n                title=\"Your Diamond vs. The Average Diamond\"\n            )\n\n            st.altair_chart(chart, use_container_width=True)\n\n        except Exception as e:\n            st.error(f\"An error occurred during prediction. Please ensure the model '{MODEL_NAME}' version '{MODEL_VERSION}' is in the Model Registry and the pipeline file is in stage '@{STAGE_NAME}'. Error: {e}\")\n\nelse:\n    st.info(\"Adjust the features in the sidebar and click 'Predict Price'.\")\n\n# --- Instructions for Setup ---\nst.sidebar.markdown(\"---\")\nst.sidebar.info(\n    \"\"\"\n    **Setup Instructions:**\n    1. Run the `setup.sql` script from the [GitHub repository](https://github.com/Snowflake-Labs/sfguide-intro-to-machine-learning-with-snowflake-ml-for-python) to create the database, schema, and tables.\n    2. Run through the notebooks in the repository to train the model, create the preprocessing pipeline, and deploy them.\n    3. Ensure this Streamlit app is running in a Snowflake environment with access to the created database and schema.\n    \"\"\"\n)",
   "execution_count": null
  }
 ]
}