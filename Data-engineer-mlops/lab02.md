# Comment charger des fichiers CSV depuis un stage vers Snowflake Notebooks üìÅ

Dans cet exemple, nous allons montrer comment charger un fichier CSV depuis un stage et cr√©er une table avec Snowpark.

Commen√ßons par utiliser la commande `get_active_session` afin d‚Äôobtenir la variable de contexte de session pour travailler avec Snowpark :

```python
from snowflake.snowpark.context import get_active_session
session = get_active_session()
# Add a query tag to the session. This helps with troubleshooting and performance monitoring.
session.query_tag = {"origin":"sf_sit-is", 
                     "name":"notebook_demo_s3", 
                     "version":{"major":1, "minor":0},
                     "attributes":{"is_quickstart":2, "source":"notebook", "vignette":"csv_from_s3"}}
print(session)
```


## Cr√©er un stage externe

Nous allons maintenant cr√©er un stage externe qui r√©f√©rence des fichiers de donn√©es stock√©s en dehors de Snowflake. Dans cet exemple, les donn√©es sont stock√©es dans un bucket S3.

```sql

CREATE DATABASE IF NOT EXISTS CITIBIKE;

USE DATABASE CITIBIKE;

USE SCHEMA PUBLIC;

CREATE STAGE IF NOT EXISTS CITIBIKE_STAGE 
	URL = 's3://logbrain-datalake/datasets/citibike-trips-csv/';
```

## V√©rifier les fichiers pr√©sents dans le stage

Examinons les fichiers disponibles dans le stage.

```sql
LS @CITIBIKE_STAGE;
```

## Charger le fichier CSV avec Snowpark

Nous pouvons utiliser **Snowpark DataFrameReader** pour lire le fichier CSV.

En utilisant l‚Äôoption `infer_schema = True`, Snowflake d√©duira automatiquement le sch√©ma √† partir des types de donn√©es pr√©sents dans le fichier CSV, ce qui √©vite de devoir le d√©finir manuellement.

```python
# Create a DataFrame that is configured to load data from the CSV file.
df = session.read.options({"infer_schema":True}).csv('@CITIBIKE_STAGE/trips_2018_0_0_0.csv.gz')
```

```python
df
```

## Travailler avec le DataFrame Snowpark

Une fois les donn√©es charg√©es dans un DataFrame Snowpark, nous pouvons les manipuler √† l‚Äôaide de l‚ÄôAPI Snowpark DataFrame.

Par exemple, nous pouvons calculer des statistiques descriptives sur les colonnes :

```python
df.describe()
```

## √âcrire le DataFrame dans une table Snowflake

Nous pouvons enregistrer le DataFrame dans une table nomm√©e `TRIPS` puis l‚Äôinterroger en SQL.

```python
df.write.mode("overwrite").save_as_table("TRIPS")
```

```sql
-- Preview the newly created TRIPS table
SELECT * from TRIPS;
```

## Relire la table dans Snowpark

Enfin, nous pouvons relire la table dans Snowpark en utilisant la syntaxe `session.table`.

```python
df = session.table("TRIPS")
df
```

## Continuer le traitement des donn√©es

√Ä partir d‚Äôici, vous pouvez continuer √† interroger et traiter les donn√©es.

```python
df.groupBy('"start_station_name"').count()
```

## Charger les donn√©es de m√©teo:

nous allons cr√©er un nouveau stage pour les donn√©es m√©t√©o

```

USE DATABASE CITBIKE;

CREATE STAGE IF NOT EXISTS WEATHER_STAGE 
	URL = 's3://logbrain-datalake/datasets/weather-nyc-json/';
```

## V√©rifier les fichiers pr√©sents dans le stage

```
LS @WEATHER_STAGE;
```

## Charger le fichier CSV avec Snowpark

Nous pouvons utiliser Snowpark DataFrameReader pour lire le fichier JSON.

En utilisant l‚Äôoption infer_schema = True, Snowflake d√©duira automatiquement le sch√©ma √† partir des types de donn√©es pr√©sents dans le fichier JSON, ce qui √©vite de devoir le d√©finir manuellement.

```
# Create a DataFrame that is configured to load data from the json file.
df = session.read.options({"infer_schema":True}).csv('@WEATHER_STAGE/hourlyData-2018-1.json.gz')
```

```
df
```
## Travailler avec le DataFrame Snowpark
 
```
df.describe()
```

## √âcrire le DataFrame dans une table Snowflake

```
df.write.mode("overwrite").save_as_table("WEATHER_JSON")
```

```
-- Preview the newly created weather table
SELECT * from WEATHER_JSON;
```

## Relire la table dans Snowpark

```
df = session.table("WEATHER_JSON")
df
```

##  Process the JSON data

```
USE DATABASE CITBIKE;

CREATE TABLE IF NOT EXISTS weather as
select 
   v:"coco"::STRING as "coco" ,
   v:"country"::STRING as "country",
   v:"dwpt"::FLOAT as "dwpt",
   v:"elevation"::STRING as "elevation",
   v:"icao"::STRING as "icao",
   v:"latitude"::DECIMAL as "latitude",
   v:"longitude"::DECIMAL as "longitude",
   v:"name"::STRING as "name",
   v:"obsTime"::TIMESTAMP as "obsTime",
   v:"prcp"::STRING as "prcp" ,
   v:"pres"::DECIMAL as "pres",
   v:"region"::STRING as "region",
   v:"rhum"::STRING as "rhum",
   v:"snow"::STRING as "snow",
   v:"station"::STRING as "station",
   v:"temp"::DECIMAL "temp",
   v:"timezone"::STRING as "timezone",
   v:"tsun"::STRING as "tsun",
   v:"wdir"::STRING as "wdir",
   v:"weatherCondition"::STRING as "weatherCondition",
   v:"wmo"::STRING as "wmo",
   v:"wpgt"::STRING as "wpgt",
   v:"wspd"::DECIMAL as "wspd"
 from WEATHER_JSON;

```

```
-- Preview the newly created weather table
SELECT * from WEATHER;
```

## Relire la table dans Snowpark

```
df = session.table("WEATHER")
df
```

## Nettoyage des ressources

Suppression de la table et du stage cr√©√©s dans cet exemple :

```sql
-- Teardown table and stage created as part of this example
DROP TABLE TRIPS;
DROP TABLE WEATHER;
DROP TABLE WEATHER_JSON;
DROP STAGE CITIBIKE_STAGE;
DROP DATABASE CITIKIE;
```
## Conclusion

Dans cet exemple, nous avons vu comment charger des fichiers CSV depuis un stage externe afin de traiter et interroger les donn√©es dans un notebook √† l‚Äôaide de Snowpark.
