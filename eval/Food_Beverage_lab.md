# Workshop â€“ Data-Driven Marketing Analytics avec Snowflake et Streamlit

## Contexte du workshop

AnyCompany Food & Beverage (entreprise fictive) est un fabricant de produits alimentaires et de boissons prÃ©sent sur le marchÃ© depuis plus de 25 ans. En 2025, lâ€™entreprise se trouve Ã  un tournant critique de son histoire.

MalgrÃ© une rÃ©putation solide et une distribution internationale de produits premium, AnyCompany fait face Ã  une **baisse des ventes sans prÃ©cÃ©dent** sur le dernier exercice fiscal, combinÃ©e Ã  une **rÃ©duction de 30 % de son budget marketing**.

Dans le mÃªme temps, le marchÃ© est devenu beaucoup plus concurrentiel avec lâ€™arrivÃ©e de marques *digital-first* et de startups en vente directe au consommateur (D2C). Ces nouveaux acteurs proposent des produits comparables Ã  des prix infÃ©rieurs de 5 Ã  15 %, sâ€™appuyant sur des chaÃ®nes logistiques optimisÃ©es et des stratÃ©gies marketing fortement pilotÃ©es par la donnÃ©e.
RÃ©sultat : la part de marchÃ© dâ€™AnyCompany est passÃ©e de **28 % Ã  22 % en seulement huit mois**.

![alt text](../images/fb1.png)


## Initiative de transformation digitale

Face Ã  cette situation, le PDG a lancÃ© une **initiative de transformation digitale**, plaÃ§ant le **marketing data-driven** au cÅ“ur de la stratÃ©gie de redressement.

Il confie cette mission Ã  **Sarah**, Senior Marketing Executive, qui constitue une petite Ã©quipe transverse et spÃ©cialisÃ©e composÃ©e de:
* Data Engineer
* Data Analyst 
* Business Analyst

### Objectif business

Lâ€™Ã©quipe doit :

* Inverser la tendance Ã  la baisse des ventes
* Atteindre une **augmentation de 10 points de part de marchÃ© (de 22 % Ã  32 %) dâ€™ici le T4 2025**
* OpÃ©rer dans un contexte de **budget marketing rÃ©duit**

Le succÃ¨s repose sur une exploitation rapide et efficace des donnÃ©es existantes afin de cibler les produits et segments Ã  plus fort potentiel.

## Parcours du lab

Ce lab reproduit les premiÃ¨res Ã©tapes de cette transformation data-driven, depuis la prÃ©paration des donnÃ©es jusquâ€™Ã  lâ€™analyse business, en sâ€™appuyant sur **Snowflake** comme plateforme analytique centrale.

## PrÃ©requis

- Compte Snowflake dâ€™essai (**Ã©tudiants : 120 jours**) â€“ inscrivez-vous avec une adresse mail valide , sociÃ©tÃ© **MBAESG**, rÃ´le **Ã‰tudiant**, **Edition : Enterprise**, **Cloud : AWS**, **RÃ©gion : us-west-2**.
- lien pour crÃ©er votre compte:  https://signup.snowflake.com/?trial=student&cloud=aws&region=us-west-2&utm_source=handsonessentials&utm_campaign=uni-dww# 
- Connaissances de Bases en SQL (DDL/DML) et objets (database, schema, tableâ€¦).  
- Connaissances des formats **CSV** et **JSON**.

**Bon Ã  savoir â€“ CrÃ©dits**  
Conservez **Auto-Suspend** activÃ© et utilisez une **taille dâ€™entrepÃ´t raisonnable** pour Ã©viter dâ€™Ã©puiser vos crÃ©dits.  

![alt text](../images/fb2.png)

# Phase 1 â€“ Data Preparation & Ingestion

## Objectif

Mettre en place un socle de donnÃ©es fiable dans Snowflake en chargeant lâ€™ensemble des fichiers CSV et JSON fournis et en les rendant exploitables pour les analyses futures.

Cette phase correspond au travail dâ€™une Ã©quipe **Data Engineering / Analytics Engineering**.

## Environnement technique

Nous utilisons **Snowflake** comme plateforme analytique.

* Les donnÃ©es sources sont stockÃ©es dans **Amazon S3**: "s3://logbrain-datalake/datasets/food-beverage/"
* Snowflake est utilisÃ© pour :

  * Le chargement des donnÃ©es
  * Le stockage analytique
  * Les requÃªtes SQL


## Contexte mÃ©tier

En tant que membres de lâ€™Ã©quipe Data Engineering, vous devez prÃ©parer des tables permettant de **corrÃ©ler les performances de vente avec les actions marketing et promotionnelles**.

Les donnÃ©es proviennent de plusieurs domaines mÃ©tier : ventes, promotions, marketing, clients, logistique, inventaire et service client.


## Fichiers fournis

Les fichiers suivants sont mis Ã  disposition pour le lab :

* `customer_demographics.csv` â€“ donnÃ©es dÃ©mographiques clients
* `customer_service_interactions.csv` â€“ interactions avec le service client
* `financial_transactions.csv` â€“ transactions de ventes
* `promotions-data.csv` â€“ donnÃ©es de promotions
* `marketing_campaigns.csv` â€“ campagnes marketing
* `product_reviews.csv` â€“ avis et notes produits
* `inventory.json` â€“ niveaux de stock
* `store_locations.json` â€“ informations gÃ©ographiques des magasins
* `logistics_and_shipping.csv` â€“ donnÃ©es logistiques
* `supplier_information.csv` â€“ informations fournisseurs
* `employee_records.csv` â€“ donnÃ©es organisationnelles

## Travail demandÃ© â€“ Phase 1

### Ã‰tape 1 â€“ PrÃ©paration de lâ€™environnement Snowflake

* CrÃ©er une base de donnÃ©es dÃ©diÃ©e (ex. `ANYCOMPANY_LAB`)
* CrÃ©er deux schÃ©mas `BRONZE` et `SILVER`
    - `BRONZE` pour les donnÃ©es brutes
    - `SILVER` pour les donnÃ©es nettoyÃ©es
* CrÃ©er un stage Snowflake pour le chargement des fichiers

### Ã‰tape 2 â€“ CrÃ©ation des tables

Pour chaque fichier :

* Identifier la structure (colonnes, types)
* CrÃ©er la table correspondante dans le schÃ©ma `BRONZE`
* Choisir des types adaptÃ©s aux usages analytiques


### Ã‰tape 3 â€“ Chargement des donnÃ©es

* Charger les fichiers avec `COPY INTO`
* VÃ©rifier les volumes chargÃ©s
* Corriger les Ã©ventuelles erreurs de parsing



### Ã‰tape 4 â€“ VÃ©rifications

* VÃ©rifier le nombre de lignes
* Inspecter un Ã©chantillon (`SELECT * LIMIT 10`)
* Identifier les colonnes clÃ©s (IDs, dates, produits, rÃ©gions)

### Ã‰tape 5 â€“

CrÃ©er des tables nettoyÃ©es, cohÃ©rentes et exploitables dans le schÃ©ma **SILVER** Ã  partir des donnÃ©es brutes du schÃ©ma **BRONZE**.

Pour chaque table dans le schÃ©ma **BRONZE** :

1. Nettoyer les donnÃ©es :

* Gestion des valeurs manquantes

* Suppression ou traitement des doublons

2. Harmoniser les formats :

* Dates

3. Appliquer des rÃ¨gles de qualitÃ© :

* Valeurs positives pour les montants

CrÃ©er les nouvelles tables nettoyÃ©es dans le schÃ©ma **SILVER**

**Exemples** :

RAW.financial_transactions â†’ SILVER.financial_transactions_clean

RAW.promotions_data â†’ SILVER.promotions_clean

### RÃ©sultat attendu

* Une base Snowflake opÃ©rationnelle
* Une table par fichier
* Des donnÃ©es propres et exploitables
* Un socle prÃªt pour la Phase 2

# Phase 2 â€“ Exploration des donnÃ©es et analyses business

## Objectif

Explorer les donnÃ©es chargÃ©es dans Snowflake afin de :

* Comprendre leur contenu et leur qualitÃ©
* Identifier des tendances et corrÃ©lations
* Produire des **insights business exploitables pour le marketing**

Cette phase correspond au travail dâ€™une Ã©quipe **Business Analyst / Data Analyst**.

## Travail demandÃ© â€“ Phase 2

### Partie 2.1 â€“ ComprÃ©hension des jeux de donnÃ©es

Pour chaque table du schÃ©ma `SILVER` :

* Identifier le pÃ©rimÃ¨tre mÃ©tier
* Identifier les colonnes clÃ©s
* VÃ©rifier volumes et pÃ©riodes couvertes
* VÃ©rifier valeurs manquantes et anomalies

### Partie 2.2 â€“ Analyses exploratoires descriptives

Ã€ lâ€™aide de requÃªtes SQL Snowflake :

* Analyse de lâ€™Ã©volution des ventes dans le temps
* Performance par produit, catÃ©gorie et rÃ©gion
* RÃ©partition des clients par segments dÃ©mographiques

### Partie 2.3 â€“ Analyses business transverses

1. **Ventes et promotions**

   * Comparaison ventes avec / sans promotion
   * SensibilitÃ© des catÃ©gories aux promotions

2. **Marketing et performance commerciale**

   * Lien campagnes â†” ventes
   * Identification des campagnes les plus efficaces

3. **ExpÃ©rience client**

   * Impact des avis produits sur les ventes
   * Influence des interactions service client

4. **OpÃ©rations et logistique**

   * Ruptures de stock
   * Impact des dÃ©lais de livraison


## Phase 3 â€“ Data Product & Machine Learning

Lâ€™objectif de cette phase est de transformer les analyses exploratoires en actifs data rÃ©utilisables, puis de dÃ©velopper des modÃ¨les Machine Learning permettant de soutenir concrÃ¨tement les dÃ©cisions marketing.

Ã€ lâ€™issue de cette phase, vous devrez Ãªtre capable de :

* Concevoir un data product analytique (Une table/view qui contient les KPIs nÃ©cessaires pour piloter l'activitÃ©) prÃªt Ã  Ãªtre consommÃ©

* Construire et Ã©valuer des modÃ¨les ML orientÃ©s marketing

* Relier les rÃ©sultats techniques Ã  des cas dâ€™usage business concrets

Cette phase correspond au travail dâ€™une Ã©quipe Analytics Engineering & Data Science.

* **Contexte**:

Ã€ lâ€™issue de la Phase 2, lâ€™Ã©quipe analytics a identifiÃ© plusieurs insights clÃ©s :

* Les promotions influencent significativement certaines catÃ©gories

* Certains segments clients prÃ©sentent un fort potentiel de croissance

* Les performances varient fortement selon les rÃ©gions et les campagnes

Ces constats doivent maintenant Ãªtre industrialisÃ©s sous forme :

* de tables analytiques stables

* de features rÃ©utilisables

* de modÃ¨les ML exploitables par les Ã©quipes marketing

* **Travail demandÃ© â€“ Phase 3**

**Partie 3.1 â€“ CrÃ©ation du Data Product**

CrÃ©er un produit data centralisÃ© combinant ventes, promotions, marketing et dimensions clÃ©s, prÃªt Ã  Ãªtre consommÃ© par des outils analytiques et des modÃ¨les ML.

**TÃ¢ches**:

1. Concevoir une ou plusieurs tables analytiques dans Snowflake (schÃ©ma ANALYTICS) :

* Table ventes enrichies

* Table promotions actives

* Table clients enrichis

2. Mettre en place :

* Des clÃ©s de jointure claires

* Une granularitÃ© maÃ®trisÃ©e (vente, client, produit, date)

* Une documentation du schÃ©ma et des champs

3. VÃ©rifier :

* La cohÃ©rence mÃ©tier

**Partie 3.2 â€“ Feature Engineering** --> OPTIONNELLE

PrÃ©parer des features ML Ã  partir du data product.

**Exemples de features attendues**

* Nombre de promotions actives au moment de lâ€™achat

* Historique dâ€™achat client (frÃ©quence, panier moyen)

* Variables temporelles (saisonnalitÃ©, rÃ©cence)

**Partie 3.3 â€“ ModÃ©lisation Machine Learning** --> OPTIONNELLE

DÃ©velopper des modÃ¨les ML pour soutenir les dÃ©cisions marketing.

**Cas dâ€™usage proposÃ©s (au choix)**

1. Segmentation clients

* Clustering basÃ© sur comportement dâ€™achat

* Identification de segments Ã  fort potentiel

2. ModÃ¨le de propension Ã  lâ€™achat

* ProbabilitÃ© quâ€™un client achÃ¨te un produit

3. RÃ©ponse aux promotions

* PrÃ©dire lâ€™impact dâ€™une promotion sur les ventes


**Partie 3.4 â€“ InterprÃ©tation et recommandations business**

Pour chaque modÃ¨le :

* Ã‰valuer les performances (metrics adaptÃ©es)

* InterprÃ©ter les rÃ©sultats (features importantes)

* Traduire les rÃ©sultats en recommandations marketing concrÃ¨tes


## Livrable attendu â€“ Projet GitHub

Le livrable des phase 1, 2 et 3 doit Ãªtre un **projet GitHub structurÃ©**, contenant :

### 0. SQL ETL

* Les requÃªtes SQL pour le chargement des donnÃ©es
* Les requÃªtes SQL pour le nettoyage des donnÃ©es
* Scripts stockÃ©s dans `sql/`
* RequÃªtes commentÃ©es et exÃ©cutables dans Snowflake

### 1. SQL analytique

* Une requÃªte SQL par analyse business
* Scripts stockÃ©s dans `sql/`
* RequÃªtes commentÃ©es et exÃ©cutables dans Snowflake

### 2. Visualisations Streamlit

* Une page Streamlit par type d'analyse
* Code stockÃ© dans `streamlit/`
* Visualisations claires et orientÃ©es dÃ©cision

### 3. Visualisations Streamlit

* Scripts SQL de crÃ©ation des tables analytiques (analytics/)
* Documentation du data product
* Notebooks ou scripts Python (ml/)
* Un document dÃ©diÃ© (ml_insights.md) contenant les rÃ©sultats des modÃ¨les et leur interprÃ©tation mÃ©tier.

### 4. SynthÃ¨se des constats business

Un document (`README.md` ou `business_insights.md`) prÃ©sentant :

* Les constats clÃ©s
* Leur interprÃ©tation mÃ©tier
* Leur impact potentiel sur la stratÃ©gie marketing

### Structure attendue

```
project-name/
â”œâ”€â”€ sql/
|   â”œâ”€â”€ Load_data.sql
|   â”œâ”€â”€ clean_data.sql
â”‚   â”œâ”€â”€ sales_trends.sql
â”‚   â”œâ”€â”€ promotion_impact.sql
â”‚   â””â”€â”€ campaign_performance.sql
â”œâ”€â”€ streamlit/
â”‚   â”œâ”€â”€ sales_dashboard.py
â”‚   â”œâ”€â”€ promotion_analysis.py
â”‚   â””â”€â”€ marketing_roi.py
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ customer_segmentation.ipynb
â”‚   â”œâ”€â”€ purchase_propensity.ipynb
â”‚   â””â”€â”€ promotion_response_model.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ business_insights.md
```

âš ï¸ **Attention**:
Le projet doit Ãªtre rÃ©alisÃ© en groupe, avec une rÃ©partition Ã©quitable des tÃ¢ches. Les livrables identiques entre groupes seront considÃ©rÃ©s comme du plagiat et sanctionnÃ©s en consÃ©quence.

### ModalitÃ©s de soumission

Le rendu du projet doit Ãªtre envoyÃ© par email avec les Ã©lÃ©ments suivants :

**Objet de lâ€™email**:  MBAESG_EVALUATION_ARCHITECTURE_BIGDATA_2026

**Contenu de lâ€™email**

Veuillez inclure :

* ğŸ”— Le lien vers votre dÃ©pÃ´t GitHub contenant lâ€™ensemble du projet (SQL, Streamlit, documentation)

* ğŸ” Les accÃ¨s Ã  votre compte Snowflake, incluant :

        - URL du compte Snowflake

        - Nom dâ€™utilisateur

        - Mot de passe

**Adresse de soumission** : ğŸ“§ axel@logbrain.fr
